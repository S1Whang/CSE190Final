# CSE190Final
Samuel Whang A11409386
CSE 190 - Robotics
Final Project: Q Learning
	For the final project, I implemented path planning using perfect state information and q-learning. Before starting on this project I first modified one the existing configuration.json files to create a new map that would be used for my qlearning implementation. This map would be a 3x5 uncyclic world with pits along the northern and southern border and the goal at the far right cell of the map. I also modified my mdp file from the last project so that I would be able to retrieve the policy map from the mdp object when calling the getMap method and use it in my qlearning file object. After making sure those two were properly working I set out to create a qlearning class.
 	In the qlearning map I used the read_config() function that was used in the last 2 programming assignments. This allowed me to pass in my own parameters quickly and I did not have to create my own json parser. In my qlearning configuration file I included a move_list, a map_size, and a list of goals, pits, walls and start states. I also added an alpha, gamma and epsilon variable that I would use in the calculations for the q-values. After creating the world map and adding obstacles I began on the robot simulation.
	For q-learning to work properly, there needs to be an agent that learns the features of the worlds through iterative simulation. In other words, the robot that I was simulating in my qlearning map needed to learn about its environment by moving around and recording what it achieved. To do this, the robot would save its progress by rewriting the dictionary of state,action tuples with the values it calculated from making each move. The robot would continue to rewrite values until it somehow reached its goal or landed into a pit.  At the end of several iterations the robot should have had sufficient enough experience with the environment and the values recorded in the dictionary should reflect correct values that have been calculated by the robot. If these values after even a few iterations were incorrect then I knew that my robot and implementation was incorrect. In this way I was able to debug and correct my program incrementally. 
	After finishing my project, I wanted a way to output my results that would be both visually appealing and also easy to read. In my printMap function I formatted values taken from  the qlearning dictionary into strings that would, in the end, create a visual map on the console screen. I also added a map that would show the position of the robot as it simulated its movement across the world environment and another map that would show the best policies outputted from the mdp file. In this way I could confirm that the values of the qlearning, the robot position, and the policy map all reflected the correct output at the end of the simulations. 
Notes:
	I kept this project simple for several reasons. First, I did not use ros packages to build my qlearning implementation because I was working alone and I did not think I would have time to write all the necessary items for the ros package. Second, I wanted to use TKinter to create a gui for my maps that would really stand out but sadly I was not able to implement this feature in either. However, my time with this project was mostly spent on making sure my qlearning implementation was correct. There might be a few corner cases that I have missed but I am sure that it works correctly. Qlearning was interesting to implement and the ascii map output was also fun to make.
Link to Github:
-	To run the program just type in: python qlearn.py
